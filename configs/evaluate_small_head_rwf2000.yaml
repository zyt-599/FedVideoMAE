experiment: evaluate_small_head_rwf2000
output_dir: FedVideomae_DP/evaluation_results/small_head_rwf2000
seed: 42

data:
  root: FedVideomae_DP/dataset/RWF-2000/evaluate
  num_frames: 16
  frame_stride: 4
  size: 224
  batch_size: 32
  num_workers: 8
  partitions: null

model:
  model_name: MCG-NJU/videomae-base
  pretrained: false  # wrapper ignores this flag; loading depends on pretrained_checkpoint
  pretrained_checkpoint:  FedVideomae_DP/runs/central_weak_finetune_rwf2000_epsilon_10/model_best.pth
  mode: head
  peft:
    use_lora: true
    lora_r: 16
    lora_alpha: 32
    lora_dropout: 0.12
    target_modules: [q_proj, k_proj, v_proj, out_proj, fc1, fc2]
    
  head:
    num_classes: 2
    type: mlp
    hidden_dim: 1536
    num_layers: 2
    dropout: 0.2

logging:
  log_every: 20
